[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skin Lesion Classification Task",
    "section": "",
    "text": "In this assignment, I was tasked with utilizing the PyTorch libraries to train and evaluate a set of deep learning models. The assignment revolved around two distinct approaches:\nTransfer learning approach: In this approach, I leveraged pre-trained models as a starting point. Transfer learning involves utilizing the knowledge gained from training on a large dataset (typically ImageNet) and applying it to a different but related task, such as classifying skin lesions. By building upon the pre-trained models, I aimed to benefit from their learned features and adapt them to improve the performance of my skin lesion classifier.\nBuilding a classifier model from scratch: In addition to the transfer learning approach, I was required to implement my own classifier model from scratch. This involved designing and training a deep learning model specifically tailored for classifying skin lesions. I had the freedom to experiment with different architectures, layers, and optimization techniques to create an effective and accurate classifier.\nBoth approaches provided valuable insights into the field of deep learning and allowed me to explore different strategies for developing a skin lesion classifier.",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#visualization-and-evaluation-of-models",
    "href": "index.html#visualization-and-evaluation-of-models",
    "title": "Skin Lesion Classification Task",
    "section": "Visualization and Evaluation of Models",
    "text": "Visualization and Evaluation of Models\n\n[EN]\nAfter completing the model training, I visually analyzed and evaluated the performance of the trained models. These observations were included in my report. The following components were expected:\nVisualizations:\nLoss Graph: Displayed the training and validation loss curves for each model. These graphs provided insights into the training progress and how close the models were to convergence over the epochs.\nConfusion Matrix: Presented a confusion matrix for each model, illustrating the classification performance across different skin lesion categories. This visual assessment helped in understanding the model’s ability to correctly predict and distinguish between different classes.\nSample Images: I included a selection of test set samples. For each model, I showcased five correctly classified test samples and five incorrectly classified test samples. This visual representation provided an intuitive understanding of the model’s strengths and weaknesses.\nEvaluation:\nComparison of Results: I compared and analyzed the performance of the transfer learning models and the model trained from scratch. To assess the effectiveness of these models in multi-category classification, I used metrics such as accuracy, precision, recall, and F1-score. These metrics were summarized and compared in a table, providing a concise overview and allowing for easy comparison and analysis. By examining the loss curves, confusion matrices, and sample images, I gained insights into the learning patterns, class separability, and error tendencies of the models. Additionally, the evaluation of accuracy metrics and the comparison table provided a quantitative assessment of the models’ classification capabilities. This comprehensive evaluation helped me analyze the strengths and weaknesses of different approaches and compare their effectiveness in skin lesion classification.\n\n\nModellerin Görselleştirilmesi ve Değerlendirilmesi\n\n[TR]\nModel eğitimlerini tamamladıktan sonra, eğitilen modellerin performanslarını görsel olarak analiz ettim ve değerlendirdim. Raporumda bu gözlemler yer aldı. Beklenen bileşenler aşağıdaki gibidir:\nGörselleştirmeler:\nKayıp Grafiği (Loss Graph): Her bir model için eğitim ve doğrulama kayıplarını gösteren grafikler oluşturuldu. Bu grafikler, modellerin eğitim sürecindeki ilerlemelerini ve her epoch boyunca ne kadar yaklaştıklarını göstermesi açısından önemliydi.\nKarışıklık Matrisi (Confusion Matrix): Her modelin sınıflandırma performansını gösteren karışıklık matrisi sunuldu. Bu matris, modellerin farklı cilt lezyonu kategorilerini ne derece doğru sınıflandırabildiğini görsel olarak değerlendirmeye olanak tanıdı.\nÖrnek Görseller: Test setinden alınan örnek görüntüler sunuldu. Her model için doğru sınıflandırılmış beş test örneği ve yanlış sınıflandırılmış beş test örneği gösterildi. Bu görsel temsil, modellerin güçlü ve zayıf yönlerini vurgulamak için önemli bir sezgisel anlayış sağladı.\nDeğerlendirme:\nSonuçların Karşılaştırılması: Transfer öğrenme modelleri ile sıfırdan eğitilen modelin performansını karşılaştırdım ve analiz ettim. Çok kategorili sınıflandırma görevinde modellerin etkinliğini değerlendirmek için doğruluk (accuracy), kesinlik (precision), geri çağırma (recall), ve F1 skoru gibi metrikleri kullandım. Bu metrikler, modellerin performansını özetleyen ve karşılaştırmalı analiz için kolaylık sağlayan bir tablo halinde sunuldu. Kayıp eğrilerini, karışıklık matrislerini ve örnek görüntüleri görsel olarak inceleyerek, modellerin öğrenme eğilimlerini, sınıf ayrışabilirliğini ve hata yapma eğilimlerini gözlemleyebildim. Ayrıca, doğruluk metrikleri ve performans karşılaştırma tablosu, modellerin sınıflandırma yeteneklerine dair nicel bir değerlendirme sundu. Bu kapsamlı değerlendirme, farklı yaklaşımların güçlü ve zayıf yönlerini analiz etmeme ve cilt lezyonu sınıflandırmasındaki etkinliklerini karşılaştırmama yardımcı oldu.",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#report",
    "href": "index.html#report",
    "title": "Skin Lesion Classification Task",
    "section": "Report",
    "text": "Report\n\n[EN]\n\nCustom Design CNN Model: In this report, I provided a detailed description of the convolutional neural network (CNN) model I built from scratch. I explained each layer in the model, its configurations, and the specific design choices I made. Additionally, I included a block diagram to visually illustrate the data flow and the connections between different layers within the model. This diagram provided a clearer understanding of the overall architecture.\n\nWhile designing the CNN, I experimented with various configurations of convolutional layers, pooling layers, and fully connected layers. By adjusting parameters like the number of layers, filter sizes, and the number of neurons in fully connected layers, I aimed to optimize the model’s performance. Image preprocessing steps such as data augmentation and normalization also played a key role in improving the model’s effectiveness.\n\nTest Results & Visualizations: In this section, I presented the test results for both the transfer learning models and the model I built from scratch. This section included:\n\nLoss Graphs: I displayed graphs showing the changes in training and validation losses over time, which helped evaluate the progress and convergence of each model during training. Confusion Matrices: I presented confusion matrices for each model to analyze their ability to correctly classify different categories of skin lesions. Sample Images: I included examples from the test set that were both correctly and incorrectly classified, which provided visual insights into the models’ strengths and weaknesses. Performance Comparison Table: I created a table comparing the accuracy, precision, recall, and F1-scores of all the models, enabling a clear comparison of their results.\n\nConclusion and Future Work: I summarized the key findings and conclusions from my experiments. I observed that transfer learning methods, especially with pre-trained models, tend to outperform custom-built models in terms of both speed and accuracy, particularly when working with limited datasets. However, building a model from scratch offered more control over the architecture and allowed for creative solutions specific to the task.\n\nFor future work, I plan to experiment with larger datasets and deeper, more complex architectures to further enhance model performance. Additionally, extending the classification to more detailed categories of skin lesions and adding more classes could be an interesting direction for future research.\n\n\nRapor\n\n[TR]\n\nÖzel Tasarım CNN Modeli: Bu raporda, sıfırdan oluşturduğum konvolüsyonel sinir ağı (CNN) modelimin mimarisini detaylı bir şekilde açıkladım. Modelde yer alan her bir katmanı, katmanların yapılandırmalarını ve yaptığım tasarım tercihlerini ayrıntılı olarak belirttim. Ayrıca, modelin katmanlar arasındaki veri akışını ve katmanlar arası bağlantıları görsel olarak gösteren bir blok diyagramı sundum. Bu diyagram, mimarinin genel yapısını daha iyi anlamayı sağladı.\n\nCNN mimarisinde farklı konvolüsyonel katmanlar, pooling katmanları ve tam bağlı katmanları denedim. Bu denemelerde katman sayısı, filtre boyutları ve tam bağlı katmanlardaki nöron sayıları gibi değişkenler üzerinde oynayarak en iyi performansı elde etmeye çalıştım. Özellikle veri çoğaltma ve normalizasyon gibi görüntü ön işleme adımlarıyla modelin performansını iyileştirdim.\n\nTest Sonuçları ve Görselleştirmeler: Raporumda, hem transfer öğrenme yöntemleriyle eğittiğim modellerin hem de sıfırdan oluşturduğum modelin test sonuçlarını sundum. Bu bölümde şu detaylar yer aldı:\n\nKayıp Grafikleri: Eğitim ve doğrulama süreçlerindeki kayıpların nasıl değiştiğini gösteren grafikler ile modellerin eğitim süreçlerindeki ilerlemeyi ve yakınsamalarını değerlendirdim. Karışıklık Matrisleri: Her bir model için karışıklık matrislerini göstererek, modellerin farklı cilt lezyonu sınıflarını doğru sınıflandırma yeteneklerini analiz ettim. Örnek Görseller: Test setinden doğru ve yanlış sınıflandırılmış örnekleri sundum. Bu örnekler modellerin güçlü ve zayıf yanlarını görsel olarak anlamamı sağladı. Performans Karşılaştırma Tablosu: Her modelin doğruluk, kesinlik, geri çağırma ve F1 skorlarını içeren bir tablo ile sonuçların karşılaştırmasını yaptım.\n\nSonuç ve Gelecek Çalışmalar: Deneyimlerimden elde ettiğim ana bulguları ve sonuçları özetledim. Transfer öğrenme yöntemlerinin sıfırdan model oluşturmaya kıyasla daha hızlı ve daha başarılı sonuçlar verebildiğini gözlemledim, özellikle sınırlı veri setleri ile çalışırken transfer öğrenmenin etkili olduğunu fark ettim. Bununla birlikte, sıfırdan model oluşturma süreci, özellikle mimari tasarım üzerinde daha fazla kontrol imkanı sunduğundan, bu tür görevler için yaratıcı çözümler üretme fırsatı sağladı.\n\nGelecekte, daha geniş veri setleri ile daha derin ve karmaşık mimariler deneyerek model performansını artırmayı planlıyorum. Ayrıca, cilt lezyonlarının daha detaylı sınıflandırılması ve daha fazla sınıf eklenmesi, gelecekteki çalışmaların odak noktası olabilir.",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#model-1-loss-curves",
    "href": "index.html#model-1-loss-curves",
    "title": "Skin Lesion Classification Task",
    "section": "Model 1 loss curves",
    "text": "Model 1 loss curves\n\ntrain_loss_1 = [0.5386, 0.4343, 0.4542, 0.4548, 0.4845, 0.4583, 0.4698, 0.4457, 0.4188, 0.4061]\ntrain_acc_1 =  [0.7901, 0.8111, 0.7914, 0.8037, 0.8012, 0.8086, 0.8037, 0.8025, 0.8173, 0.8210]\nval_loss_1 =   [13.0946,0.8220, 0.4609, 0.4562, 5.2942, 0.6526, 0.3620, 0.4424, 0.5997, 0.6453]\nval_acc_1 =    [0.8667, 0.6111, 0.8667, 0.8333, 0.8667, 0.7333, 0.8778, 0.8778, 0.9000, 0.8778]\nplt.plot(train_loss_1, label='Model-1 Training Loss')\nplt.plot(val_loss_1, label='Model-1 Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Curves of Model-1')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#model-2-loss-curves",
    "href": "index.html#model-2-loss-curves",
    "title": "Skin Lesion Classification Task",
    "section": "Model 2 loss curves",
    "text": "Model 2 loss curves\n\ntrain_loss_2 = [0.6450, 0.4924, 0.4801, 0.4401, 0.4602, 0.4094, 0.4227, 0.4044, 0.3749, 0.3800]\ntrain_acc_2 =  [0.7728, 0.8025, 0.7914, 0.8049, 0.8062, 0.8259, 0.8160, 0.8173, 0.8407, 0.8383]\nval_loss_2 =   [1.4463, 0.7435, 1.4627, 0.7477, 0.5359, 1.3895, 0.5916, 0.6277, 0.5674, 0.4910]\nval_acc_2 =    [0.8000, 0.7111, 0.7778, 0.6333, 0.8556, 0.4333, 0.8889, 0.7889, 0.8778, 0.8667]\nplt.plot(train_loss_2, label='Model-2 Training Loss')\nplt.plot(val_loss_2, label='Model-2 Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Curves of Model-2')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#model-3-loss-curves",
    "href": "index.html#model-3-loss-curves",
    "title": "Skin Lesion Classification Task",
    "section": "Model 3 loss curves",
    "text": "Model 3 loss curves\n\ntrain_loss_3 = [0.5055, 0.4295, 0.4247, 0.3726, 0.3781, 0.3361, 0.3577, 0.3788, 0.3386, 0.3262]\ntrain_acc_3 =  [0.7975, 0.8148, 0.8148, 0.8370, 0.8346, 0.8506, 0.8519, 0.8457, 0.8617, 0.8580]\nval_loss_3 =   [0.4412, 0.5671, 0.3774, 0.4966, 0.4125, 0.3548, 0.3344, 0.4833, 0.3358, 0.3409]\nval_acc_3 =    [0.8556, 0.7556, 0.8667, 0.7889, 0.8667, 0.8889, 0.9000, 0.8000, 0.9000, 0.9000]\n\nplt.plot(train_loss_3, label='Model-3 Training Loss')\nplt.plot(val_loss_3, label='Model-3 Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Curves of Mod#### Model 2 loss curvesel-3')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#confusion-matrix-classification-report-precision-recall-f1-score-and-support",
    "href": "index.html#confusion-matrix-classification-report-precision-recall-f1-score-and-support",
    "title": "Skin Lesion Classification Task",
    "section": "1.5.1- Confusion matrix, Classification Report: precision, recall, f1-score, and support",
    "text": "1.5.1- Confusion matrix, Classification Report: precision, recall, f1-score, and support\n\n1.5.1.1- Confusion matrix, Classification Report: precision, recall, f1-score, and support of the model 1\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np\n\n# Evaluation on the test set\ntest_predictions = []\ntest_labels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n\n        _, preds = torch.max(outputs, 1)\n        test_predictions.extend(preds.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntest_predictions = np.array(test_predictions)\ntest_labels = np.array(test_labels)\n\n# Compute metrics\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, test_predictions))\n\n# Generate confusion matrix\nprint(\"Confusion Matrix:\")\ncm = confusion_matrix(test_labels, test_predictions)\nprint(cm)\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.84      0.96      0.90       304\n           1       0.62      0.24      0.35        75\n\n    accuracy                           0.82       379\n   macro avg       0.73      0.60      0.62       379\nweighted avg       0.79      0.82      0.79       379\n\nConfusion Matrix:\n[[293  11]\n [ 57  18]]\n\n\n\n\n1.5.1.2- Confusion matrix, Classification Report: precision, recall, f1-score, and support of the model 2\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np\n\n# Evaluation on the test set\ntest_predictions = []\ntest_labels = []\n\nmodel_2.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model_2(images)\n\n        _, preds = torch.max(outputs, 1)\n        test_predictions.extend(preds.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntest_predictions = np.array(test_predictions)\ntest_labels = np.array(test_labels)\n\n# Compute metrics\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, test_predictions))\n\n# Generate confusion matrix\nprint(\"Confusion Matrix:\")\ncm = confusion_matrix(test_labels, test_predictions)\nprint(cm)\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.98      0.89       304\n           1       0.59      0.13      0.22        75\n\n    accuracy                           0.81       379\n   macro avg       0.70      0.56      0.55       379\nweighted avg       0.77      0.81      0.76       379\n\nConfusion Matrix:\n[[297   7]\n [ 65  10]]\n\n\n\n\n1.5.1.3- Confusion matrix, Classification Report: precision, recall, f1-score, and support of the model 3\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np\n\n# Evaluation on the test set\ntest_predictions = []\ntest_labels = []\n\nmodel_3.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model_3(images)\n\n        _, preds = torch.max(outputs, 1)\n        test_predictions.extend(preds.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntest_predictions = np.array(test_predictions)\ntest_labels = np.array(test_labels)\n\n# Compute metrics\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, test_predictions))\n\n# Generate confusion matrix\nprint(\"Confusion Matrix:\")\ncm = confusion_matrix(test_labels, test_predictions)\nprint(cm)\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.98      0.90       304\n           1       0.67      0.13      0.22        75\n\n    accuracy                           0.82       379\n   macro avg       0.74      0.56      0.56       379\nweighted avg       0.79      0.82      0.76       379\n\nConfusion Matrix:\n[[299   5]\n [ 65  10]]",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#example-of-correctly-classified-samples-and-incorrectly-classified-samples",
    "href": "index.html#example-of-correctly-classified-samples-and-incorrectly-classified-samples",
    "title": "Skin Lesion Classification Task",
    "section": "1.5.2- Example of Correctly Classified Samples and Incorrectly Classified Samples",
    "text": "1.5.2- Example of Correctly Classified Samples and Incorrectly Classified Samples\n\n1.5.2.1- Example of Correctly Classified Samples and Incorrectly Classified Samples of model 1\n\n# display images\ndef show_images(images, labels, predicted_labels, title):\n    fig, axes = plt.subplots(2, min(5, len(images)), figsize=(12, 6))\n    fig.suptitle(title, fontsize=14)\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; len(images):\n            ax.imshow(images[i].permute(1, 2, 0))\n            ax.axis('off')\n            ax.set_title(f\"True: {labels[i]}\\nPred: {predicted_labels[i]}\")\n        else:\n            ax.axis('off')\n\n# Get a batch of test images and labels\nimages, labels = next(iter(test_loader))\n\n# Move images to device\nimages = images.to(device)\n\n# Get model predictions\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(images)\n    _, preds = torch.max(outputs, 1)\n\n# Get indices of correctly and incorrectly classified images\ncorrect_indices = (preds == labels).nonzero().squeeze()\nincorrect_indices = (preds != labels).nonzero().squeeze()\n\n# Select correctly classified images\nnum_correct = min(5, len(correct_indices))\ncorrect_images = images[correct_indices][:num_correct]\ncorrect_labels = labels[correct_indices][:num_correct]\ncorrect_preds = preds[correct_indices][:num_correct]\n\n# Select incorrectly classified images\nnum_incorrect = min(5, len(incorrect_indices))\nincorrect_images = images[incorrect_indices][:num_incorrect]\nincorrect_labels = labels[incorrect_indices][:num_incorrect]\nincorrect_preds = preds[incorrect_indices][:num_incorrect]\n\n# Show the selected images\nshow_images(correct_images, correct_labels, correct_preds, \"Correctly Classified Samples\")\nshow_images(incorrect_images, incorrect_labels, incorrect_preds, \"Incorrectly Classified Samples\")\n\n# Display the images\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2.2- Example of Correctly Classified Samples and Incorrectly Classified Samples of model 2\n\n# display images\ndef show_images(images, labels, predicted_labels, title):\n    fig, axes = plt.subplots(2, min(5, len(images)), figsize=(12, 6))\n    fig.suptitle(title, fontsize=14)\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; len(images):\n            ax.imshow(images[i].permute(1, 2, 0))\n            ax.axis('off')\n            ax.set_title(f\"True: {labels[i]}\\nPred: {predicted_labels[i]}\")\n        else:\n            ax.axis('off')\n\n# Get a batch of test images and label\nimages, labels = next(iter(test_loader))\n\n# Device\nimages = images.to(device)\n\n# Get model predictions\nmodel_2.eval()\nwith torch.no_grad():\n    outputs = model_2(images)\n    _, preds = torch.max(outputs, 1)\n\n# Get indices of correctly and incorrectly classified imagess. Lieted with 5 \ncorrect_indices = (preds == labels).nonzero().squeeze()\nincorrect_indices = (preds != labels).nonzero().squeeze()\n\n# Select correctly classified images\nnum_correct = min(5, len(correct_indices))\ncorrect_images = images[correct_indices][:num_correct]\ncorrect_labels = labels[correct_indices][:num_correct]\ncorrect_preds = preds[correct_indices][:num_correct]\n\n# Select incorrectly classified images\nnum_incorrect = min(5, len(incorrect_indices))\nincorrect_images = images[incorrect_indices][:num_incorrect]\nincorrect_labels = labels[incorrect_indices][:num_incorrect]\nincorrect_preds = preds[incorrect_indices][:num_incorrect]\n\n# Show the selected images\nshow_images(correct_images, correct_labels, correct_preds, \"Correctly Classified Samples\")\nshow_images(incorrect_images, incorrect_labels, incorrect_preds, \"Incorrectly Classified Samples\")\n\n# Display the images\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2.3- Example of Correctly Classified Samples and Incorrectly Classified Samples of model 3\n\n# display images\ndef show_images(images, labels, predicted_labels, title):\n    fig, axes = plt.subplots(2, min(5, len(images)), figsize=(12, 6))\n    fig.suptitle(title, fontsize=14)\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; len(images):\n            ax.imshow(images[i].permute(1, 2, 0))\n            ax.axis('off')\n            ax.set_title(f\"True: {labels[i]}\\nPred: {predicted_labels[i]}\")\n        else:\n            ax.axis('off')\n\n# Get a batch of test images and labels a\nimages, labels = next(iter(test_loader))\n\n# choose a device for runnin\nimages = images.to(device)\n\n# Get model prediction\nmodel_3.eval()\nwith torch.no_grad():\n    outputs = model_3(images)\n    _, preds = torch.max(outputs, 1)\n\n# Get indices of correctly andincorrectly classified images. Limeted with 5\ncorrect_indices = (preds == labels).nonzero().squeeze()\nincorrect_indices = (preds != labels).nonzero().squeeze()\n\n# Select correctly classified images\nnum_correct = min(5, len(correct_indices))\ncorrect_images = images[correct_indices][:num_correct]\ncorrect_labels = labels[correct_indices][:num_correct]\ncorrect_preds = preds[correct_indices][:num_correct]\n\n# Select incorrectly classified images\nnum_incorrect = min(5, len(incorrect_indices))\nincorrect_images = images[incorrect_indices][:num_incorrect]\nincorrect_labels = labels[incorrect_indices][:num_incorrect]\nincorrect_preds = preds[incorrect_indices][:num_incorrect]\n\n# Show the selected images\nshow_images(correct_images, correct_labels, correct_preds, \"Correctly Classified Samples\")\nshow_images(incorrect_images, incorrect_labels, incorrect_preds, \"Incorrectly Classified Samples\")\n\n# Display the images\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#i-thought-the-model-did-not-prerdict-any-1-but-when-i-look-at-a-few-more-pictures-and-than-i-see-it.-it-was-just-a-coincidence.",
    "href": "index.html#i-thought-the-model-did-not-prerdict-any-1-but-when-i-look-at-a-few-more-pictures-and-than-i-see-it.-it-was-just-a-coincidence.",
    "title": "Skin Lesion Classification Task",
    "section": "I thought the model did not prerdict any 1 but when I look at a few more pictures and than I see it. It was just a coincidence.",
    "text": "I thought the model did not prerdict any 1 but when I look at a few more pictures and than I see it. It was just a coincidence.\n\n# I was looking why model is not predicting 1 but it seems it predict 1.\ntest_correct = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n\n        x, preds = torch.max(outputs, 1)## I thought the model did not prerdict any 1 but when I look at a few more pictures and than I see it. It was just a coincidence.\n        print(x)\n        test_correct += torch.sum(preds == labels.data)\n\ntest_acc = test_correct.double()/ len(test_dataset)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\ntensor([ 0.0937,  2.1015,  3.6010,  2.4113,  1.9821,  1.7362,  1.2605,  0.8603,\n         1.7896,  5.0396,  5.2531, 50.8374,  2.6454,  1.1652,  1.7367,  0.9046,\n         1.6091,  1.8293,  2.7449,  3.1041,  0.3036,  3.9430,  1.1789,  1.3454,\n         2.3033,  1.4861,  2.7959,  1.0077,  1.6401,  2.3750,  0.9363,  0.5674])\ntensor([2.5817, 0.9873, 1.3215, 1.5310, 2.2378, 1.4115, 1.6017, 0.3990, 1.4902,\n        0.4133, 1.6955, 0.1113, 0.2570, 1.0877, 1.4184, 0.9930, 2.4602, 2.8013,\n        1.4107, 1.5000, 0.6790, 1.3751, 2.0535, 0.2843, 0.2412, 2.1956, 1.1011,\n        0.7443, 1.7058, 1.3067, 1.9446, 0.0089])\ntensor([ 0.8349,  0.3133,  1.3670,  3.3524,  3.5384,  0.2645,  1.5857,  3.4304,\n        -0.4512, -0.3601,  0.1897,  2.4914, -0.0942,  0.1356,  2.0123, -0.0498,\n         0.3288,  1.7425,  0.9330,  1.4010,  1.8567,  1.6810,  2.2702,  0.1694,\n        -0.0960,  2.9003,  0.4485, -0.2392,  2.4882,  0.7698,  0.7547,  1.2207])\ntensor([1.5043, 1.0157, 1.2613, 3.0613, 0.2635, 1.1433, 1.2761, 0.9536, 0.7417,\n        1.4428, 2.0577, 1.7440, 0.3943, 2.7956, 1.1047, 1.3567, 1.5960, 1.1970,\n        2.2807, 1.1686, 1.8387, 0.1625, 1.2236, 1.0147, 0.8317, 0.5501, 0.3410,\n        0.8569, 1.2969, 0.1061, 1.9943, 0.7279])\ntensor([ 0.8545,  0.8127,  1.2135,  4.1292,  1.6904,  0.9264, -0.2408,  1.1856,\n         1.8489, 87.4981,  1.4319,  5.9103, 20.6343, -0.2789, 16.8895,  4.6065,\n         3.1078, 42.5545,  4.8674, 43.5815, 79.5309, 29.3269,  4.9300, 19.0257,\n        29.0336, 17.5688,  1.6997,  9.8101,  2.4479, 24.4624, 16.1752, 79.0457])\ntensor([101.2009,   3.0091,  39.1507,   3.8212,   5.1475,   7.5182,  32.3012,\n         35.0839,   1.6544,   1.4746,   1.1124,   9.7696,  88.0368,  77.9080,\n          9.4977,  10.5813,  11.4202,   0.9530,   0.6935,   3.1215,   1.0884,\n          1.7153,   1.3366,   0.5971,   1.6596,   1.7849,   0.3785,   2.0836,\n          1.2670,   1.7869,   1.4597,   1.7633])\ntensor([2.2995, 2.1081, 2.3648, 1.3362, 1.8183, 1.3060, 0.8119, 1.6908, 2.5642,\n        3.0446, 0.6321, 2.1565, 2.3138, 1.0462, 1.9045, 2.1485, 0.3267, 2.2342,\n        1.2976, 1.7640, 2.1171, 2.6122, 1.3607, 1.1441, 2.0413, 1.1485, 1.0785,\n        1.8160, 1.3603, 1.3085, 2.5379, 1.6335])\ntensor([2.0473, 0.9899, 0.3702, 1.3680, 2.0031, 1.6296, 0.4149, 1.6536, 0.7690,\n        1.8846, 1.7417, 0.9885, 0.6942, 2.6520, 0.6851, 1.3870, 0.9074, 2.0471,\n        1.0763, 1.4366, 2.3153, 0.6569, 1.2559, 1.0868, 0.7112, 1.0420, 1.3834,\n        1.2426, 1.1627, 1.6373, 1.2623, 1.0946])\ntensor([1.1277, 3.2547, 1.3066, 2.1425, 1.4845, 2.0286, 1.7710, 2.6210, 0.5834,\n        1.3838, 1.3659, 1.0842, 0.8454, 1.5239, 2.1263, 2.7938, 1.1597, 1.8536,\n        1.4809, 1.0453, 1.6131, 0.9616, 1.7907, 2.0963, 0.8363, 0.1313, 1.2552,\n        2.6176, 3.3169, 0.9542, 0.6622, 1.2455])\ntensor([0.9635, 2.2767, 2.1864, 1.4981, 0.0593, 2.2318, 2.6336, 1.8114, 2.5235,\n        2.0843, 1.5744, 0.9195, 3.2828, 1.3356, 0.4239, 0.6484, 1.5783, 0.6217,\n        2.2783, 1.1738, 0.7524, 1.6400, 0.4848, 1.1484, 2.0387, 0.7545, 1.7639,\n        2.5285, 1.9325, 0.0385, 1.1693, 0.6602])\ntensor([2.2509, 0.7275, 1.0698, 0.9149, 1.8650, 1.4176, 1.0211, 1.6861, 1.6237,\n        2.1444, 1.6880, 0.9437, 2.0135, 1.0426, 1.4953, 0.3369, 1.3841, 1.7618,\n        1.7872, 0.5326, 3.0921, 1.6711, 1.5916, 0.6846, 1.4660, 1.1372, 1.4315,\n        2.6840, 1.2566, 2.0639, 1.0983, 2.5201])\ntensor([1.1694, 2.6556, 1.7032, 1.5781, 1.4159, 0.9517, 2.5772, 2.2222, 2.1363,\n        3.0863, 0.3688, 1.4876, 1.6623, 2.0530, 1.3271, 2.1487, 1.0181, 0.6534,\n        2.2778, 0.9635, 1.3127, 0.2119, 1.9036, 0.0226, 2.7261, 1.7074, 0.9662])\nTest Accuracy: 0.7995\n\n\n\n# same here too.\ntest_correct = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n\n        _, preds = torch.max(outputs, 1)\n        \n        print(torch.sum(preds == labels.data))\n        test_correct += torch.sum(preds == labels.data)\n\ntest_acc = test_correct.double()/ len(test_dataset)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(len(test_dataset))\n\ntensor(25)\ntensor(23)\ntensor(24)\ntensor(24)\ntensor(23)\ntensor(30)\ntensor(25)\ntensor(28)\ntensor(27)\ntensor(25)\ntensor(26)\ntensor(23)\nTest Accuracy: 0.7995\n379",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#loss-curve-of-cnn-model.",
    "href": "index.html#loss-curve-of-cnn-model.",
    "title": "Skin Lesion Classification Task",
    "section": "Loss curve of CNN model.",
    "text": "Loss curve of CNN model.\n\nTrain Loss: 0.8324, 0.5082, 0.4658, 0.4420, 0.4376, 0.4262, 0.3820, 0.3857, 0.3818, 0.3714\nTrain Acc:  0.7457, 0.8062, 0.8049, 0.8111, 0.8210, 0.8160, 0.8259, 0.8272, 0.8407, 0.8469\nVal Loss:   ## I thought the model did not prerdict any 1 but when I look at a few more pictures and than I see it. It was just a coincidence.0.6008, 0.9038, 1.7440, 1.2597, 3.0783, 3.4121, 3.4937, 8.7321, 0.6095, 2.0479\nVal Acc:    0.8778, 0.8667, 0.8667, 0.8889, 0.8889, 0.8778, 0.8778, 0.8778, 0.8444, 0.8556\n\n\nimport matplotlib.pyplot as plt\n\ntrain_loss_1 = [0.8324, 0.5082, 0.4658, 0.4420, 0.4376, 0.4262, 0.3820, 0.3857, 0.3818, 0.3714]\ntrain_acc_1 =  [0.7457, 0.8062, 0.8049, 0.8111, 0.8210, 0.8160, 0.8259, 0.8272, 0.8407, 0.8469]\nval_loss_1 =   [0.6008, 0.9038, 1.7440, 1.2597, 3.0783, 3.4121, 3.4937, 8.7321, 0.6095, 2.0479]\nval_acc_1 =    [0.8778, 0.8667, 0.8667, 0.8889, 0.8889, 0.8778, 0.8778, 0.8778, 0.8444, 0.8556]\n\nplt.plot(train_loss_1, label='CNN Model Training Loss')\nplt.plot(val_loss_1, label='CNN Model Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Curve for CNN Model')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#confusion-matrix-and-classification-report-of-cnn-model",
    "href": "index.html#confusion-matrix-and-classification-report-of-cnn-model",
    "title": "Skin Lesion Classification Task",
    "section": "Confusion matrix and Classification Report of CNN Model",
    "text": "Confusion matrix and Classification Report of CNN Model\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np\n\n# Evaluation on the test set\ntest_predictions = []\ntest_labels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n\n        _, preds = torch.max(outputs, 1)\n        test_predictions.extend(preds.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntest_predictions = np.array(test_predictions)\ntest_labels = np.array(test_labels)\n\n# Compute metrics\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, test_predictions))\n\n# Generate confusion matrix\nprint(\"Confusion Matrix:\")\ncm = confusion_matrix(test_labels, test_predictions)\nprint(cm)\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.81      0.98      0.89       304\n           1       0.46      0.08      0.14        75\n\n    accuracy                           0.80       379\n   macro avg       0.64      0.53      0.51       379\nweighted avg       0.74      0.80      0.74       379\n\nConfusion Matrix:\n[[297   7]\n [ 69   6]]\n\n\n\n# display images\ndef show_images(images, labels, predicted_labels, title):\n    fig, axes = plt.subplots(2, min(5, len(images)), figsize=(12, 6))\n    fig.suptitle(title, fontsize=14)\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; len(images):\n            ax.imshow(images[i].permute(1, 2, 0))\n            ax.axis('off')\n            ax.set_title(f\"True: {labels[i]}\\nPred: {predicted_labels[i]}\")\n        else:\n            ax.axis('off')\n\n# Get a batch of test images and labels\nimages, labels = next(iter(test_loader))\n\n# Move images to device\nimages = images.to(device)\n\n# Get model predictions\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(images)\n    _, preds = torch.max(outputs, 1)\n\n# Get indices of correctly and incorrectly classified images\ncorrect_indices = (preds == labels).nonzero().squeeze()\nincorrect_indices = (preds != labels).nonzero().squeeze()\n\n# Select correctly classified images\nnum_correct = len(correct_indices)\ncorrect_images = images[correct_indices][:num_correct]\ncorrect_labels = labels[correct_indices][:num_correct]\ncorrect_preds = preds[correct_indices][:num_correct]\n\n# Select incorrectly classified images\nnum_incorrect = len(incorrect_indices)\nincorrect_images = images[incorrect_indices][:num_incorrect]\nincorrect_labels = labels[incorrect_indices][:num_incorrect]\nincorrect_preds = preds[incorrect_indices][:num_incorrect]\n\n# Show the selected images\nshow_images(correct_images, correct_labels, correct_preds, \"Correctly Classified Samples\")\nshow_images(incorrect_images, incorrect_labels, incorrect_preds, \"Incorrectly Classified Samples\")\n\n# Display the images\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "index.html#i-thought-the-model-did-not-prerdict-any-1-but-when-i-look-at-a-few-more-pictures-and-than-i-see-it.-it-was-just-a-coincidence.-and-also-i-look-to-the-confusion-matrix-and-it-shows-us-69-6-predictions-results-are-1.",
    "href": "index.html#i-thought-the-model-did-not-prerdict-any-1-but-when-i-look-at-a-few-more-pictures-and-than-i-see-it.-it-was-just-a-coincidence.-and-also-i-look-to-the-confusion-matrix-and-it-shows-us-69-6-predictions-results-are-1.",
    "title": "Skin Lesion Classification Task",
    "section": "^^ ^^ I thought the model did not prerdict any 1 but when I look at a few more pictures and than I see it. It was just a coincidence. And also I look to the confusion matrix and it shows us 69 + 6 predictions results are 1.",
    "text": "^^ ^^ I thought the model did not prerdict any 1 but when I look at a few more pictures and than I see it. It was just a coincidence. And also I look to the confusion matrix and it shows us 69 + 6 predictions results are 1.",
    "crumbs": [
      "Skin Lesion Classification Task"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Fevzi KILAS",
    "section": "",
    "text": "https://niexche.github.io\nhttps://fevzikilas.github.io\nhttps://github.com/fevzikilas\nhttps://huggingface.co/NIEXCHE\n\n@niexche",
    "crumbs": [
      "Fevzi KILAS"
    ]
  }
]